#!/bin/bash

# This bash script wraps the functionalities of web_crawler.py

me=$(basename $0)

# Get the application location
script_dir=$(python  <(cat <<HERE
import os, inspect, network_crawler
print(os.path.abspath(inspect.getfile(network_crawler)))
HERE
))
script_dir="${script_dir/__init__.py[c]/}"
script=${script_dir}web_crawler.pyc


# Exit codes
OK=0
FAIL=1

# Usage functions
function synopsis {
    synopsis=$(python $script -h 2>&1 | sed -n '/web_crawler.py/,/^$/p' | sed -e 's/usage: \(.*\)/\1/')
    synopsis=${synopsis/.py[c]/}
    echo $synopsis
}

function get_installation() {
    echo "$script_dir"
}

function get_options {
	pos_args="positional arguments"
	opt_args="optional arguments"
    export pattern_one=$pos_args
    export pattern_two=$opt_args
	usage_txt=$(python $script --help)

    # Get positional arguments
    script="$(cat <<"EOF"
    # Get each option
    if ((/$ENV{pattern_one}/../$ENV{pattern_two}/) &&    # Print anything between
        !(/$ENV{pattern_one}/||/$ENV{pattern_two}/)) {   # the two patterns
        $opt = $_;
    }
    if ($opt =~ /^\s{3,}/) {
        # Remove traling or leading spaces
        $opt =~ s/^\s+|\s+\n$//g;
        print("\t" x 4 . $opt);
    }
    elsif ($opt =~ /^\s+/) {
        # Remove traling or leading spaces
        $opt =~ s/^\s+|\s+\n$//g;
        # Add double hypen for positional arguments
        $opt =~ s/^(.+)$/--$1/;
        # Readjust the spacing
        $opt =~ s/\s{2,}/\t\t/;
        if ($opt =~ /--.{1,5}\t/) {
            $opt =~ s/(.+)\t/$1\t\t/;
        }
        if ($opt) {
            print("$opt");
        }
    }
EOF
)"
	echo "$usage_txt" | perl -ne "$script"

    # Get optional arguments
    script="$(cat <<"EOF"
    my $pad = " " x 4;

    if ($can_print) {
        my $opt = $_;
        # Remove traling or leading spaces
        $opt =~ s/^[\s\t]+|[\s\t]+$//g;
        # Multiline comment
        if ($_ =~ /^\s+[a-zA-Z]+/) {
            if ($_ !~ /^\s+$/) {
                if ($no_pad eq 1) {
                    if ($prev) {
                        printf("\n${prev}\t\t%s\n", $opt);
                        $prev = "";
                    }
                    $no_pad = 0;
                }
                else {
                    printf("%s$opt\n", $pad x 8);
                }
            }
            next;
        }
        else {

            # Readjust the spacing
            $opt =~ s/(.+)(\s{2,})/$1\t\t/;

            # Add padding
            $opt =~ s/^(.+)$/${pad}${1}/;

            $no_pad = 0;

            if ($opt =~ /\t[\w\d]+/) {
                # Add newline
                if ($opt !~ /\n$/) {
                    $opt =~ s/^(.+)$/$1\n/;
                }
                if ($opt !~ /^\s+$/) {
                    print("\n$opt");
                }
            }
            else {
                $no_pad = 1;
                $prev = $opt;
            }
        }
    }
    if (/$ENV{pattern_two}/) {
        $can_print = 1;
    }
EOF
)"
    echo "$usage_txt" | perl -ne "$script"
}

function get_requirements {
    awk '{printf("%s, ",$0)}' ${script_dir}requirements.txt | sed 's/, $//'
}

function get_version {
    version="$(python $script --version 2>&1)"
    echo ${version/.py[c]/}
}

function usage {
    bold=$(tput bold)
    underline=$(tput smul)
    reset=$(tput sgr0)
	cat <<-END
${bold}NAME${reset}

    ${me} -- networks' international calling costs web crawler

${bold}SYNOPSIS${reset}

    $(synopsis)

    (See the OPTIONS section for alternate option syntax with long option names.)

${bold}DESCRIPTION${reset}

    ${underline}${me}${reset} is a simple web crawler for networks' international calling costs.

    The script queries a network's website directly to obtain a list of calling costs.
    These costs are: based on country zones, per minute and only related to calls to
    a foreign country from the UK.

    ${underline}${me}${reset} requires a JSON file containing:
        - operator name
        - operator URL
        - list of country zones
        - list of actions for the Selenium driver
        - load time: the maximum amount of time to wait for a page element to get loaded
        - sleep time: the amount of time of sleeping after each action

    A log file can be used to track the crawling process, if not supplied then all the
    messages will be printed to STDOUT (default).

${bold}OPTIONS${reset}

    $(get_options)

${bold}EXAMPLES${reset}

    Run with logging at debug level, printing to STDOUT:

        web_crawler --data operators.json -l "debug"

    Print the output to STDOUT in JSON format:

        web_crawler --data operators.json --json

    Print the output to a file in JSON format:

        web_crawler --data operators.json --json --out file.json

    Log the process to a specific directory:

        web_crawler --data operators.json --lod-dir /var/log/

    Dry-run mode, only prints the logging messages:

        web_crawler --data operators.json -q

${bold}REQUIREMENTS${reset}

    Python modules: $(get_requirements).

${bold}EXIT CODES${reset}

    0 - Success
    1 - Failure

${bold}INSTALLATION${reset}

    The application is installed in:

        $(get_installation)

${bold}AUTHOR${reset}

    Luigi Riefolo <luigi.riefolo@gmail.com>

${bold}LICENSE${reset}

    The MIT License (MIT).

${bold}VERSION${reset}

    $(get_version).

END
}

# Script invoked with no command-line args
if [[ $# == 0 ]]
then
	usage
	exit $FAIL
fi

optspec=":h-:"
while getopts "$optspec" optchar; do
    case "${optchar}" in
        -)
            case "${OPTARG}" in
                *)
                    :
                    ;;
            esac;;
        h)
            usage
            exit $OK
            ;;
    esac
done

python "$script" $@
ret=$?
if [[ $ret -ne 0 ]];
then
    echo "See the documentation: web_crawler -h"
fi
